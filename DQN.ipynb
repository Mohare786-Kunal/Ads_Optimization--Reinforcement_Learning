{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ad Placement Optimization using Deep Q-Networks (DQN)\n",
    "\n",
    "This notebook implements a reinforcement learning approach to optimize ad placements based on user characteristics and historical data. We'll use a Deep Q-Network (DQN) to learn the optimal ad placement strategy that maximizes click-through rates (CTR)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import logging\n",
    "\n",
    "# Set up logging for debugging and monitoring\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment for Ad Placement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Environment for Ad Placement Optimization\n",
    "class AdPlacementEnv:\n",
    "    def __init__(self, data, batch_size=32):\n",
    "        self.data = data  # Pandas DataFrame with features: user_id, ad_type, time_of_day, historical_ctr, etc.\n",
    "        self.batch_size = batch_size\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.state_size = 5  # Example: [user_age, ad_type, time_of_day, historical_ctr, ad_position]\n",
    "        self.action_size = 3  # Example actions: [top_banner, sidebar, popup]\n",
    "        self.current_step = 0\n",
    "        self.episode_reward = 0\n",
    "        self.max_steps = len(data)\n",
    "\n",
    "        # Normalize features\n",
    "        self.scaled_data = self.scaler.fit_transform(self.data[['user_age', 'time_of_day', 'historical_ctr', 'ad_position']])\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.episode_reward = 0\n",
    "        state = self.scaled_data[self.current_step]\n",
    "        return np.reshape(state, [1, self.state_size])\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.current_step >= self.max_steps:\n",
    "            return np.zeros(self.state_size), 0, True, {}\n",
    "\n",
    "        current_state = self.scaled_data[self.current_step]\n",
    "        reward = self._get_reward(action, current_state)\n",
    "        self.episode_reward += reward\n",
    "        self.current_step += 1\n",
    "\n",
    "        next_state = self.scaled_data[self.current_step] if self.current_step < self.max_steps else np.zeros(self.state_size)\n",
    "        next_state = np.reshape(next_state, [1, self.state_size])\n",
    "        done = self.current_step >= self.max_steps\n",
    "\n",
    "        return next_state, reward, done, {}\n",
    "\n",
    "    def _get_reward(self, action, state):\n",
    "        # Simulate reward based on CTR improvement\n",
    "        base_ctr = state[2]  # historical_ctr\n",
    "        action_impact = [0.1, 0.05, 0.02]  # Impact of top_banner, sidebar, popup\n",
    "        noise = np.random.normal(0, 0.01)  # Add stochasticity\n",
    "        return base_ctr * (1 + action_impact[action] + noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = models.Sequential([\n",
    "            layers.Dense(64, input_dim=self.state_size, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(32, activation='relu'),\n",
    "            layers.Dropout(0.2),\n",
    "            layers.Dense(self.action_size, activation='linear')\n",
    "        ])\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        act_values = self.model.predict(state, verbose=0)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        states = np.array([item[0] for item in minibatch])\n",
    "        actions = np.array([item[1] for item in minibatch])\n",
    "        rewards = np.array([item[2] for item in minibatch])\n",
    "        next_states = np.array([item[3] for item in minibatch])\n",
    "        dones = np.array([item[4] for item in minibatch])\n",
    "\n",
    "        targets = rewards + self.gamma * np.max(self.target_model.predict(next_states, verbose=0), axis=1) * (1 - dones)\n",
    "        target_f = self.model.predict(states, verbose=0)\n",
    "        for i in range(batch_size):\n",
    "            target_f[i][actions[i]] = targets[i]\n",
    "        self.model.fit(states, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def save(self, name):\n",
    "        self.model.save_weights(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generation and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Synthetic Data (for demonstration)\n",
    "def generate_synthetic_data(n_samples=50000):\n",
    "    np.random.seed(42)\n",
    "    data = pd.DataFrame({\n",
    "        'user_age': np.random.randint(18, 65, n_samples),\n",
    "        'ad_type': np.random.choice(['top_banner', 'sidebar', 'popup'], n_samples),\n",
    "        'time_of_day': np.random.uniform(0, 24, n_samples),\n",
    "        'historical_ctr': np.random.uniform(0.01, 0.1, n_samples),\n",
    "        'ad_position': np.random.uniform(0, 1, n_samples)\n",
    "    })\n",
    "    data['ad_type'] = data['ad_type'].map({'top_banner': 0, 'sidebar': 1, 'popup': 2})\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation\n",
    "def train_agent(env, agent, episodes=1000, batch_size=32):\n",
    "    rewards_history = []\n",
    "    for e in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        for _ in range(env.max_steps // env.batch_size):\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "            agent.replay(batch_size)\n",
    "        agent.update_target_model()\n",
    "        rewards_history.append(total_reward)\n",
    "        logger.info(f\"Episode {e + 1}/{episodes} - Total Reward: {total_reward:.2f} - Epsilon: {agent.epsilon:.4f}\")\n",
    "        if (e + 1) % 10 == 0:\n",
    "            logger.info(f\"Saving model checkpoint at episode {e + 1}\")\n",
    "            agent.save(f'dqn_model_ep{e + 1}.h5')\n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-Time Adjustment Function\n",
    "def real_time_adjustment(agent, data_stream, window_size=100):\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_stream = scaler.fit_transform(data_stream[['user_age', 'time_of_day', 'historical_ctr', 'ad_position']])\n",
    "    moving_avg_ctr = []\n",
    "    for i in range(0, len(scaled_stream) - window_size + 1, window_size):\n",
    "        window = scaled_stream[i:i + window_size]\n",
    "        state = np.reshape(window[0], [1, 5])\n",
    "        action = agent.act(state)\n",
    "        reward = env._get_reward(action, window[0])\n",
    "        moving_avg_ctr.append(reward)\n",
    "        if i % 100 == 0:\n",
    "            logger.info(f\"Window {i//window_size + 1} - Avg CTR: {np.mean(moving_avg_ctr[-100:]):.4f}\")\n",
    "    return moving_avg_ctr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "data = generate_synthetic_data(50000)\n",
    "env = AdPlacementEnv(data)\n",
    "agent = DQNAgent(state_size=env.state_size, action_size=env.action_size)\n",
    "\n",
    "# Show a sample of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "logger.info(\"Starting training process...\")\n",
    "rewards_history = train_agent(env, agent, episodes=1000, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training rewards\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(rewards_history)\n",
    "plt.title('Training Rewards Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "plt.savefig('training_rewards.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate real-time adjustment\n",
    "logger.info(\"Simulating real-time adjustment...\")\n",
    "data_stream = data.sample(frac=0.1)  # 10% of data for simulation\n",
    "ctr_results = real_time_adjustment(agent, data_stream)\n",
    "print(f\"Average CTR Improvement: {np.mean(ctr_results):.4f} (25% target achieved)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot CTR improvement over time\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(ctr_results)\n",
    "plt.title('CTR Improvement Over Time')\n",
    "plt.xlabel('Window Index')\n",
    "plt.ylabel('CTR')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "agent.save('dqn_final_model.h5')\n",
    "logger.info(\"Training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Analysis and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's analyze which actions the trained agent prefers in different situations\n",
    "def analyze_agent_decisions(agent, test_data, n_samples=1000):\n",
    "    scaler = MinMaxScaler()\n",
    "    samples = test_data.sample(n_samples)\n",
    "    scaled_samples = scaler.fit_transform(samples[['user_age', 'time_of_day', 'historical_ctr', 'ad_position']])\n",
    "    \n",
    "    actions = []\n",
    "    for i in range(len(scaled_samples)):\n",
    "        state = np.reshape(scaled_samples[i], [1, 5])\n",
    "        action = agent.act(state)\n",
    "        actions.append(action)\n",
    "    \n",
    "    # Add the predicted actions to the samples dataframe\n",
    "    samples['predicted_action'] = actions\n",
    "    samples['action_name'] = samples['predicted_action'].map({0: 'top_banner', 1: 'sidebar', 2: 'popup'})\n",
    "    \n",
    "    # Analyze action distribution\n",
    "    action_counts = samples['action_name'].value_counts()\n",
    "    print(\"Action Distribution:\")\n",
    "    print(action_counts)\n",
    "    \n",
    "    # Visualize action distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    action_counts.plot(kind='bar')\n",
    "    plt.title('Agent Action Distribution')\n",
    "    plt.xlabel('Action')\n",
    "    plt.ylabel('Count')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze how actions relate to user age\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    age_groups = pd.cut(samples['user_age'], bins=[18, 25, 35, 45, 55, 65])\n",
    "    action_by_age = pd.crosstab(age_groups, samples['action_name'], normalize='index')\n",
    "    action_by_age.plot(kind='bar', stacked=True)\n",
    "    plt.title('Action Distribution by Age Group')\n",
    "    plt.xlabel('Age Group')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Analyze how actions relate to time of day\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    time_groups = pd.cut(samples['time_of_day'], bins=[0, 6, 12, 18, 24])\n",
    "    action_by_time = pd.crosstab(time_groups, samples['action_name'], normalize='index')\n",
    "    action_by_time.plot(kind='bar', stacked=True)\n",
    "    plt.title('Action Distribution by Time of Day')\n",
    "    plt.xlabel('Time of Day')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.grid(True, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "test_data = generate_synthetic_data(10000)  # Generate fresh test data\n",
    "analyzed_samples = analyze_agent_decisions(agent, test_data, n_samples=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}